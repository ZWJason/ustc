% !TeX root = ../main.tex

\ustcsetup{
  keywords = {
    卷积神经网络, 嵌入式, 知识蒸馏, 网络架构优化，模型压缩
  },
  keywords* = {
    Convolutional Neural Networks, Embedded, Deployment，Knowledge Distillation, Re-parameterization, Networks Optimization
  },
}

\begin{abstract}

随着卷积神经网络的不断发展，因其优异的精度和泛化性能，目前已被应用于各类工业生产环境中。然而，卷积神经网络巨大的参数规模和计算量阻碍了其在存储和计算资源有限的嵌入式设备上的部署。例如，在Imagenet数据上训练的VGG模型尺寸超过500MB，浮点计算总量超过20G。并且，目前各类高实时性高精度的智能AI应用场景如流水线残品检测、工厂自动化控制、星载设备遥感识别等，要求将卷积神经网络直接部署在嵌入式设备进行计算推理，以消除由于网络数据传输而带来的计算延迟和数据泄露问题。因此，在尽量不影响模型精度的前提下，如何尽可能的将卷积神经网络模型进行加速和压缩后再部署到嵌入式设备上是目前亟需解决的问题。

针对卷积模型在算力有限嵌入式设备上推理效率低、耗时长的问题，本文对卷积神经网络的计算特点和模式进行分析发现，卷积网络的拓扑结构会显著影响模型推理时的效率。首先，多分支拓扑结构的网络在进行计算时，为保证各分支计算流结算结果的独立性，需要暂存多份数据副本和中间结果进行计算。对于计算并行能力较差和算力受限的嵌入式设备来说，反倒增加了额外的内访访问、内核切换开销，降低了推理效率。其次，多分支拓扑结构的网络模型中存在多个细粒度的卷积算子。细粒度算子的执行效率不如粗粒度算子，如点卷积(1x1卷积核)的计算过程不是向量运算，故无法使用体系结构所提供的向量运算指令。因此，为提升模型在嵌入式设备上的推理计算效率，本文提出一种高效的神经网络架构优化方法，重新设计Resnet网络的残差结构并提出三种融合算子，以解耦训练时网络架构和推理时网络架构，充分利用多分支拓扑结构的学习能力和单分支高效推理的优点，加速网络推理。

针对卷积模型存储尺寸体积大和过参数化(over-parameterized)，难以部署在存储资源有限的嵌入式设备上的问题，本文对主流模型压缩方法中的知识蒸馏进行分析发现，现有的知识蒸馏框架缺乏一个有效的机制来提升模型压缩率和蒸馏效率。首先，知识蒸馏压缩训练的过程是静态的，即在压缩训练过程中，不会根据紧凑学生网络的学习能力变化而动态调整，这让压缩训练耗时为常规训练的两倍以上。其次，紧凑学生网络和教师网络之间存在容量差距问题，强迫紧凑学生网络学习尺寸过大(over-large)的教师网络的输出，反倒会让紧凑学生网络的精度和压缩比率出现下降。因此，为提升知识蒸馏的模型压缩比率和蒸馏效率，本文提出了一种高效的自适应知识蒸馏优化方法。在传统知识蒸馏框架中引入辅助教学网络和自适应器，根据紧凑网络学习能力的变化，在训练组(train batch)粒度上动态的调整压缩蒸馏策略，提升压缩比率减少训练耗时。

本文在嵌入式平台NVIDIA TX2上和NVIDIA V100全面的评估了高效的卷积神经网络架构和自适应知识蒸馏优化方法。测试实验结果表明，在仅损失了微小的模型精度的代价下，加速推理2.08-4.3倍，模型压缩比率和训练速度分别提升了XXX-XXX倍和XX-XXX倍。
\end{abstract}

\begin{abstract*}

With the development of convolution neural networks(CNNs), it has been applied in various industrial production environments because of its excellent accuracy and generalization performance. However, the large parameter size and computational complexity of convolution networks hinder their deployment on embedded devices with limited storage and computing resources. For example, a VGG model trained on Imagenet data is more than 500 MB in size, and the total floating-point computation is more than 20G. At present, all kinds of intelligent AI applications with high real-time and high accuracy, such as pipeline residual detection, factory automation control, remote sensing identification of space-borne equipment, require that convolution neural network be deployed directly in embedded devices for computing inference to eliminate the calculation delay and data leakage problems caused by network data transmission. Therefore, how to accelerate and compress the convolution neural network model as much as possible before deploying it to embedded devices is an urgent problem to be solved on the premise of minimizing the impact of model accuracy.

To solve the inefficiency and time-consuming problem of convolution model inference on embedded devices with limited computing power, this paper analyses the calculation characteristics and modes of convolution neural network and finds that the topological structure of convolution network can significantly affect the efficiency of model inference. First, when computing a network with multibranch topology, in order to ensure the independence of the settlement results of each branch's computing flow, it is necessary to temporarily store multiple copies of data and intermediate results for calculation. For embedded devices with poor computing parallelism and limited computing power, an additional cost of internal access and kernel switching is added, which reduces the inference efficiency. Secondly, there are many fine-grained convolution operators in the network model of multibranch topology. Fine-grained operators perform less efficiently than coarse-grained operators, such as point convolution (1x1 convolution kernel), which is not a vector operation and therefore cannot use the vector operation instructions provided by the architecture. Therefore, in order to improve the efficiency of inference calculation of models on embedded devices, this paper presents an efficient optimization method of neural network architecture, redesigns the residual structure of Resnet network and proposes three fusion operators to decouple the network architecture during training and the network architecture during inference, making full use of the learning ability of multi-branch topology and the advantages of single-branch efficient inference. Accelerate network reasoning.

Due to the large storage size and over-parameterized storage of convolution models, it is difficult to deploy them on embedded devices with limited storage resources. In this paper, the analysis of knowledge distillation in the main model compression methods reveals that the existing knowledge distillation framework lacks an effective mechanism to improve the model compression rate and distillation efficiency. First, the process of knowledge distillation compression training is static, that is, during the compression training process, it will not dynamically adjust according to the changes of learning ability of the compact student network, which makes the compression training more than twice as long as the regular training. Secondly, there is a capacity gap between the compact student network and the teacher network. Forcing the output of the compact student network to learn too large will reduce the accuracy and compression ratio of the compact student network. Therefore, in order to improve the model compression ratio and distillation efficiency of knowledge distillation, an efficient adaptive knowledge distillation optimization method is presented. In the traditional knowledge distillation framework, an auxiliary teaching network and an adapter were introduced. Based on the changes of the compact network learning ability, the compressed distillation strategy was dynamically adjusted on the granularity of the training group (train batch) to increase the compression ratio and reduce the training time.

This paper comprehensively evaluates the efficient convolution neural network architecture and adaptive knowledge distillation optimization methods on the embedded platform NVIDIA TX2 and NVIDIA V100. The test results show that, at the cost of losing only a small model accuracy, the accelerated inference is 2.08-4.3 times, the model compression ratio and the training speed are increased by XXX-XXX times and XX-XXX times, respectively.

\end{abstract*}
