% !TeX root = ../main.tex

\chapter{总结与展望}

近年来，深度神经网络在许多领域，尤其是视觉识别任务中取得了巨大的成功。然而，现有的深度神经网络模型计算成本高，内存密集，阻碍了它们在内存资源较低的设备或在有严格延迟要求的应用中部署。随着深度神经网络的研究与发展，目前已成为许多智能系统的基本工具。与此同时，这些网络的计算复杂度和资源消耗也在不断增加，现有的深度神经网络模型需要消耗大量的计算资源且占用大量内存，从而组队网络的部署造成了重大挑战，特别是在实时应用或资源受限设备上的部署。因此，网络的优化和加速已经成为目前深度学习研究的一个热门话题。本文以深度卷积神经网络为研究对象，从不同的角度探索研究深度卷积神经网络在嵌入式平台下部署的优化与加速算法，在有限的计算资源的前提下尽可能的发挥出深度学习的性能优势，摆脱硬件和功耗的束缚，扩展深度学习的应用场景。

\section{全文总结}

由于在嵌入式设备和边缘设备计算和存储资源有限，现有多数卷积神经网络在该场景下的部署和应用局限性比较大。本文主要针对嵌入式环境下计算和存储资源受限的问题，对卷积神经网络在嵌入式设备上的部署加速和优化进行研究。

首先，广泛调研目前的神经网络压缩和优化研究现状，对模型压缩和优化加速的向相关算法进行了广泛的研究，总结了该领域现有的几类主流研究方向：模型修剪、矩阵/张量分解、模型参数量化、高效网络设计、知识蒸馏、分支融合与重参数化，对各类算法和研究进行了大量的文献阅读和探索性的实验，进行一定的总结和综述。

其次，探讨了如何在低算力嵌入式设备上适配神经网络或者紧凑的网络设计，即考虑通过合理选择神经网络架构和规模压缩方案，来有效地实现约束型硬件资源上的模型部署和推理。针对此问题，借助动态网络的思想和运行时内存交换技术，本章提出了任务感知的子网络切换算法，解决了在内存约束的嵌入式设备上部署DNN模型时内存不足的问题。通过动态网络的思想为不同类的数据训练一个专属子网，并在运行时根据输入数据动态切换子网进行推理，可以有效消除这种冗余，加快推理速度，并有效减少运行时内存需求。在运行时，我们的方法检测任务类型，并将关键子网的模型参数从外部存储交换到内存。与传统的网络剪枝方法相比，该方法在保持较高的推理精度的同时，进一步减少了内存需求和推理速度。

然后，分析了多分支网络和线性网络的结构差异性与优缺点，针对具有残差结构的多分支网络，提出了针对残差结构的分支融合方法，将内存低效和低并行度的残差结构通过用重参数化技术对其分支进行融合，将多分支的ResNet合并成为一个类VGG的线性网络，提高网络部署运行时的内存效率和并行度，节省网络资源消耗，加快网络推理速度。

\section{后续工作展望}

近年来，卷积神经网络在预测精度方面发展非常快，但随着准确度提高，模型推理速度也会随着模型计算复杂度增加而变慢。因此如何在精度和速度之间取得更好的平衡一直是模型设计过程中一个非常大的挑战。此外，目标任务或者硬件平台的差异性、理论复杂度和实际速度差异性和平台或业务的额外约束都极大地加大了模型设计难度。我们之前的工作，主要基于动态模型和重参数化两个方面，他们有较大的不同，但是其共同点是将模型训练阶段和部署阶段分离，训练时和部署时模型结构并不相同，结合训练阶段和部署阶段各自的特性进行网络设计。未来的工作将主要围绕着模型训练和部署之间的特性，考虑嵌入式设备的资源局限性，着重于在部署时的效率优化，进行进一步的研究。





